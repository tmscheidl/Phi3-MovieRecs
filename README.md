# LLM-Based Movie Recommender Systems

This repository contains practical work focused on developing and evaluating multiple movie recommender system models using large language models (LLMs). The objective is to explore diverse recommendation strategiesâ€”from genre-based filtering to chain-of-thought (COT) explanation-enhanced approachesâ€”aimed at improving accuracy, diversity, and transparency in movie recommendations.

---

## ğŸ“ Repository Structure

```text
â”œâ”€â”€ scripts/                       # Python scripts for each model (S1 to S7)
â”‚   â”œâ”€â”€ S1.py
â”‚   â”œâ”€â”€ S2.py
â”‚   â”œâ”€â”€ S3.py
â”‚   â”œâ”€â”€ S4.py
â”‚   â”œâ”€â”€ S5.py
â”‚   â”œâ”€â”€ S6.py
â”‚   â””â”€â”€ S7.py
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ Plot.ipynb                # Visualizations of evaluation results
â”‚   â””â”€â”€ Movie_rec.ipynb           # Full pipeline: loading, evaluation, metrics
â”‚
â”œâ”€â”€ results/                      # Evaluation outputs
â”‚   â”œâ”€â”€ Evaluation_Metrics_Table.csv
â”‚   â”œâ”€â”€ Recall_and_NDCG_Results.csv
â”‚   â””â”€â”€ System-Level_Entropy.csv
â”‚
â”œâ”€â”€ data/                         # Preprocessed MovieLens data
â”œâ”€â”€ model/                        # Model architecture and configuration
â”œâ”€â”€ reference/                    # Academic references and citations
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # Project overview and documentation

```

---

## ğŸ§  Methodology

### 1. Data Preparation

User profiles, item metadata, and user-movie interaction histories are extracted from the MovieLens dataset. These are processed to generate user histories and candidate movie pools.

### 2. Model Implementations

Each Python script in the `scripts/` folder represents a recommendation model:

- `S1`: Baseline recommendation (e.g., popularity-based)
- `S2`: Strict genre-matching recommendations
- `S3`: Diversification-focused recommender
- `S4`: Quality-aware diversification
- `S5`: Surprise-enhanced recommender
- `S6`: Motivation reasoning using LLM
- `S7`: Chain-of-Thought (COT) reasoning with LLM

Models S6 and S7 utilize the Phi-3.5-mini-instruct LLM to produce text-based reasoning or motivational explanations for their recommendations.

### 3. Evaluation Metrics

Models are assessed using both relevance- and diversity-oriented metrics:

- Relevance:  
  - Hit Rate  
  - Average Rank  
  - Recall@5  
  - NDCG@5

- Diversity:  
  - Herfindahl-Hirschman Index (HHI)  
  - Entropy  
  - Gini Index

- System-Level Metrics:  
  - Entropy across recommendations for all users

### 4. Visualization and Analysis

The notebook `Plot.ipynb` visualizes the evaluation metrics to compare model performances in terms of trade-offs between accuracy and diversity.

---

## ğŸ“Š Results Summary

- ğŸ” Relevance:  
  The COT-based model (`S7`) demonstrates improved Hit Rate and NDCG, showing that LLM-generated explanations enhance recommendation quality.

- ğŸŒ Diversity:  
  Models `S3â€“S5` increase diversity scores by promoting novel or underrepresented items.

- ğŸ’¬ Explainability:  
  `S6` and `S7` offer reasoning or motivational narratives, increasing transparency and user trust.

- ğŸ“ˆ System-Level Entropy:  
  Indicates healthy diversity across the user base in advanced models.

---

## â–¶ï¸ Usage

1. Install required Python packages:

```bash
pip install -r requirements.txt
```

2. Run any recommendation model:

```bash
python scripts/S1.py   # Replace S1 with any model from S1 to S7
```
3. View or analyze the results:

The results will be stored in the results/ folder as CSV files:

There are some previous results:

- Evaluation_Metrics_Table.csv

- Recall_and_NDCG_Results.csv

- System-Level_Entropy.csv

You can check the visualisation of the performance using by the notebook:

```bash
jupyter notebook notebooks/Plot.ipynb
```
## ğŸ“š References

All theoretical underpinnings, architectural inspirations, and related works are cited in the reference/ folder using BibTeX and formatted citations.

## âœ… Status
âœ” All code, models, evaluation data, and visualizations are complete.

âœ” Full experimental pipeline and findings are documented.

âœ” Models are reproducible and ready for extension or adaptation.
